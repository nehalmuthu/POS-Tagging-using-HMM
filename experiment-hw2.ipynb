{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import collections"
      ],
      "metadata": {
        "id": "iFsEJqQ3r60p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_file = '/content/train'\n",
        "vocab_file = 'vocab.txt'\n",
        "model_file = 'hmm.json'\n",
        "dev_file = 'dev'\n",
        "test_file = 'test'\n",
        "threshold = 2\n",
        "tagset=set()"
      ],
      "metadata": {
        "id": "zBEtWJjWr-gC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART 1"
      ],
      "metadata": {
        "id": "_9Z356j-A6ni"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "Sl5nRF_daLiR"
      },
      "outputs": [],
      "source": [
        "# Count the occurrences of each word in the training data\n",
        "word_counts = collections.defaultdict(int)\n",
        "with open(train_file, 'r') as f:\n",
        "  for line in f:\n",
        "    line = line.strip().split('\\t')\n",
        "    if len(line) == 3:\n",
        "      tagset.add(line[2])\n",
        "      word_counts[line[1]] += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "vocab = {word: count for word, count in word_counts.items() if count >= threshold}\n",
        "\n",
        "unk_count = sum(count for word, count in word_counts.items() if count < threshold)\n",
        "unk_words = [word for word, count in word_counts.items() if count < threshold]\n",
        "unk_words2=len(unk_words)\n",
        "\n",
        "\n",
        "k=0\n",
        "# Write the vocabulary to a file\n",
        "with open(vocab_file, 'w') as f:\n",
        "  f.write(f\"<unk>\\t 0 \\t {unk_count}\\n\")\n",
        "  for word, count in sorted(word_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "    if word in vocab:\n",
        "      k=k+1\n",
        "      f.write(f\"{word}\\t{k}\\t{count}\\n\")\n",
        "\n",
        "print(f\"Threshold for unk: {threshold}\")\n",
        "print(f\"Vocabulary size: {k+1}\") # +1 for the <unk>\n",
        "print(f\"Unknown words count: {unk_count}\") \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8-7dQLjaOhx",
        "outputId": "bbb6d35f-ca7a-4829-8da2-8d856db8ebb7"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshold for unk: 2\n",
            "Vocabulary size: 23183\n",
            "Unknown words count: 20011\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART 2\n"
      ],
      "metadata": {
        "id": "D6UChGBLaQ4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def learn_hmm(train_file, vocab, model_file):\n",
        "    # Count the transitions and emissions in the training data\n",
        "    transition_counts = collections.defaultdict(int)\n",
        "    emission_counts = collections.defaultdict(int)\n",
        "    state_counts = collections.defaultdict(int)\n",
        "    start_state_counts = collections.defaultdict(int)\n",
        "    end_state_counts = collections.defaultdict(int)\n",
        "    prev_state = None\n",
        "    with open(train_file, 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.strip().split('\\t')\n",
        "            if len(line) == 3:\n",
        "                word, state = line[1], line[2]\n",
        "                state_counts[state] += 1\n",
        "                if prev_state is None:\n",
        "                    start_state_counts[state] += 1\n",
        "                if prev_state is not None:\n",
        "                    transition_counts[(prev_state, state)] += 1\n",
        "                if word in vocab:\n",
        "                    emission_counts[(state, word)] += 1\n",
        "                else:\n",
        "                    emission_counts[(state,'<unk>')] += 1\n",
        "                prev_state = state\n",
        "            else:\n",
        "                end_state_counts[prev_state] += 1\n",
        "                prev_state = None\n",
        "\n",
        "    transition = {}\n",
        "    for (prev_state, state), count in transition_counts.items():\n",
        "        transition[f\"{prev_state},{state}\"] = count / state_counts[prev_state]\n",
        "\n",
        "    for state, count in start_state_counts.items():\n",
        "        transition[f\"STARTING,{state}\"] = count / sum(start_state_counts.values())\n",
        "    \n",
        "    for state, count in end_state_counts.items():\n",
        "        transition[f\"{state},ENDING\"] = count / state_counts[state]\n",
        "    \n",
        "\n",
        "    emission = {}\n",
        "    for (state, word), count in emission_counts.items():\n",
        "        emission[f\"{state},{word}\"] = count / state_counts[state]\n",
        "\n",
        "    hmm = {\"transition\": transition, \"emission\": emission}\n",
        "    with open(\"hmm.json\", \"w\") as f:\n",
        "        json.dump(hmm, f)\n",
        "\n",
        "\n",
        "with open(vocab_file, 'r') as f:\n",
        "    vocab = {line.strip().split('\\t')[0]: int(line.strip().split('\\t')[1]) for line in f}\n",
        "\n",
        "learn_hmm(train_file, vocab, model_file)\n",
        "\n",
        "with open(model_file, 'r') as f:\n",
        "    model = json.load(f)\n",
        "    transition_params = model['transition']\n",
        "    emission_params = model['emission']\n",
        "\n",
        "print(f\"Number of transition parameters: {len(transition_params)}\")\n",
        "print(f\"Number of emission parameters: {len(emission_params)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwh-bB-naRwg",
        "outputId": "10fb7018-084d-4fdb-8c68-eca403846186"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of transition parameters: 1416\n",
            "Number of emission parameters: 30303\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART 3 - Greedy"
      ],
      "metadata": {
        "id": "VcJQumrLaUGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def greedy_decode(words, vocab, transition_params, emission_params):\n",
        "    tags = []\n",
        "    for i in range(len(words)):\n",
        "        word = words[i]\n",
        "       \n",
        "        if word not in vocab:\n",
        "            word = \"<unk>\"\n",
        "        max_tag = None\n",
        "        max_prob = -1\n",
        "        if i==0:\n",
        "          for tag in tagset:\n",
        "            transition_prob=transition_params.get(f\"STARTING,{tag}\",0.0000001)\n",
        "            #transition_prob=1\n",
        "            emission_prob = emission_params.get(f\"{tag},{word}\", 0.0000001)\n",
        "            prob = transition_prob * emission_prob\n",
        "            if prob > max_prob:\n",
        "              max_prob = prob\n",
        "              max_tag = tag\n",
        "        else:\n",
        "          for tag in tagset:\n",
        "            transition_prob=transition_params.get(f\"{tags[i-1]},{tag}\",0.0000001)\n",
        "            emission_prob = emission_params.get(f\"{tag},{word}\", 0.0000001)\n",
        "            prob = transition_prob * emission_prob\n",
        "            if prob > max_prob:\n",
        "              max_prob = prob\n",
        "              max_tag = tag\n",
        "\n",
        "        tags.append(max_tag)\n",
        "\n",
        "    return tags\n"
      ],
      "metadata": {
        "id": "uPjzjGRmaVUY"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accuracy on dev data\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9m0BnTqRaqnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(vocab_file, 'r') as f:\n",
        "    vocab = {line.strip().split('\\t')[0]: int(line.strip().split('\\t')[2]) for line in f}\n",
        "\n",
        "with open(model_file, 'r') as f:\n",
        "    model = json.load(f)\n",
        "    transition_params = model['transition']\n",
        "    emission_params = model['emission']\n",
        "\n",
        "words = []\n",
        "gold_tags = []\n",
        "with open(dev_file, 'r') as f:\n",
        "  for i, line in enumerate(f):\n",
        "    line = line.strip().split('\\t')\n",
        "    if len(line) == 3:\n",
        "      word, gold_tag = line[1], line[2]\n",
        "      words.append(word)\n",
        "      gold_tags.append(gold_tag)\n",
        "tags = greedy_decode(words, vocab, transition_params, emission_params)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for j in range(len(words)):\n",
        "  if tags[j] == gold_tags[j]:\n",
        "    correct += 1\n",
        "  total += 1\n",
        "print(correct)\n",
        "print(total)\n",
        "accuracy = correct / total\n",
        "    \n",
        "print(f\"Accuracy on dev data: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxS-un3kadGp",
        "outputId": "731a3f55-7115-4b6d-dc10-0d32bed1cd76"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "122952\n",
            "131768\n",
            "Accuracy on dev data: 0.9330945297796126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predictions on Test"
      ],
      "metadata": {
        "id": "wAkg9torau5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "words = []\n",
        "gold_tags = []\n",
        "with open(test_file, 'r') as f:\n",
        "  for line in f:\n",
        "    line = line.strip().split('\\t')\n",
        "    if len(line) == 2:\n",
        "      word = line[1]\n",
        "      words.append(word)\n",
        "\n",
        "tags = greedy_decode(words, vocab, transition_params, emission_params)\n",
        "\n",
        "greedyOutput = open(\"greedy.out\", \"w\")\n",
        "k=0\n",
        "with open(test_file, 'r') as f:\n",
        "  for line in f:\n",
        "    line = line.strip().split('\\t')\n",
        "    if len(line) == 2:\n",
        "      idx,word  = line[0], line[1]\n",
        "      tag=tags[k]\n",
        "      k=k+1\n",
        "      greedyOutput.write(f\"{idx}\\t{word}\\t{tag}\\n\")\n",
        "    else:\n",
        "      greedyOutput.write(f\"{line[0]}\\n\")    \n",
        "f.close()\n",
        "greedyOutput.close()\n"
      ],
      "metadata": {
        "id": "-ieadPhdagVJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART 4 - Viterbi"
      ],
      "metadata": {
        "id": "ajcGoiE1axL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accuracy on Dev"
      ],
      "metadata": {
        "id": "-ieWHmmSa4-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sentence wise split\n",
        "import collections\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "def viterbi_decode(words, vocab, transition_params, emission_params):\n",
        "    tags = []\n",
        "    n = len(words)\n",
        "    pi = {}\n",
        "    bp = {}\n",
        "    for i in range(n):\n",
        "        word = words[i]\n",
        "        if word not in vocab:\n",
        "            word = \"<unk>\"\n",
        "        for tag in tagset:\n",
        "            if i == 0:\n",
        "                #pi[f\"{i},{tag}\"] = np.log(transition_params.get(f\"STARTING,{tag}\", 1e-6)) + np.log(emission_params.get(f\"{tag},{word}\", 1e-6))\n",
        "                pi[f\"{i},{tag}\"] = transition_params.get(f\"STARTING,{tag}\", 1e-6) * emission_params.get(f\"{tag},{word}\", 1e-6)\n",
        "\n",
        "            else:\n",
        "                #max_prob = -1e10\n",
        "                max_prob = -1\n",
        "                max_prev_tag = None\n",
        "                for prev_tag in tagset:\n",
        "                    #prob = pi[f\"{i-1},{prev_tag}\"] + np.log(transition_params.get(f\"{prev_tag},{tag}\", 1e-6)) + np.log(emission_params.get(f\"{tag},{word}\", 1e-6))\n",
        "                    prob = pi[f\"{i-1},{prev_tag}\"] * transition_params.get(f\"{prev_tag},{tag}\", 1e-6) * emission_params.get(f\"{tag},{word}\", 1e-6)\n",
        "                    if prob > max_prob:\n",
        "                        max_prob = prob\n",
        "                        max_prev_tag = prev_tag\n",
        "                pi[f\"{i},{tag}\"] = max_prob\n",
        "                bp[f\"{i},{tag}\"] = max_prev_tag\n",
        "\n",
        "    max_prob = -1\n",
        "\n",
        "    max_end_tag = None\n",
        "    for tag in tagset:\n",
        "        #prob = pi[f\"{n-1},{tag}\"] + np.log(transition_params.get(f\"{tag},STOPPING\", 1e-6))\n",
        "        prob = pi[f\"{n-1},{tag}\"] * transition_params.get(f\"{tag},STOPPING\", 1e-6)\n",
        "\n",
        "        if prob > max_prob:\n",
        "            max_prob = prob\n",
        "            max_end_tag = tag\n",
        "\n",
        "    tags = [max_end_tag]\n",
        "    for i in range(n-1, 0, -1):\n",
        "        tags.insert(0, bp[f\"{i},{tags[0]}\"])\n",
        "    return tags\n",
        "\n",
        "def evaluate_viterbi_decoding_dev(dev_file, vocab, transition_params, emission_params):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    words = []\n",
        "    gold_tags = []\n",
        "    with open(dev_file, 'r') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            line = line.strip().split('\\t')\n",
        "            if len(line) == 3:\n",
        "                word, gold_tag = line[1], line[2]\n",
        "                words.append(word)\n",
        "                gold_tags.append(gold_tag)\n",
        "            else:\n",
        "                tags = viterbi_decode(words, vocab, transition_params, emission_params)\n",
        "                for j in range(len(words)):\n",
        "                  if tags[j] == gold_tags[j]:\n",
        "                    correct += 1\n",
        "                  total += 1\n",
        "                words = []\n",
        "                gold_tags = []\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "dev_file = 'dev'\n",
        "test_file = 'test'\n",
        "vocab_file = 'vocab.txt'\n",
        "model_file = 'hmm.json'\n",
        "\n",
        "with open(vocab_file, 'r') as f:\n",
        "    vocab = {line.strip().split('\\t')[0]: int(line.strip().split('\\t')[2]) for line in f}\n",
        "\n",
        "with open(model_file, 'r') as f:\n",
        "    model = json.load(f)\n",
        "    transition_params = model['transition']\n",
        "    emission_params = model['emission']\n",
        "\n",
        "accuracy = evaluate_viterbi_decoding_dev(dev_file, vocab, transition_params, emission_params)\n",
        "print(f\"Accuracy on dev data: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baB7JofGazQM",
        "outputId": "ad904a85-4a58-428b-c97a-dcc978c00ad3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on dev data: 0.9475449901708526\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction on test"
      ],
      "metadata": {
        "id": "AD6Y1Hu8a7vf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_viterbi_decoding_test(test_file, vocab, transition_params, emission_params):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    words = []\n",
        "    result=[]\n",
        "    with open(test_file, 'r') as f:\n",
        "        for line in f:\n",
        "          line = line.strip().split('\\t')\n",
        "          if len(line) == 2:\n",
        "            word = line[1]\n",
        "            words.append(word)\n",
        "          else:\n",
        "            tags = viterbi_decode(words, vocab, transition_params, emission_params)\n",
        "            for t in tags:\n",
        "              result.append(t)\n",
        "            words = []\n",
        "    \n",
        "    tags = viterbi_decode(words, vocab, transition_params, emission_params)\n",
        "    for t in tags:\n",
        "      result.append(t)\n",
        "    f.close()\n",
        "    \n",
        "    return result            \n",
        "\n",
        "\n",
        "tags = evaluate_viterbi_decoding_test(test_file, vocab, transition_params, emission_params)\n",
        "\n"
      ],
      "metadata": {
        "id": "sP0sSWmva2tQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "viterbiOutput = open(\"viterbi.out\", \"w\")\n",
        "k=0\n",
        "with open(test_file, 'r') as f:\n",
        "  for line in f:\n",
        "    line = line.strip().split('\\t')\n",
        "    if len(line) == 2:\n",
        "      idx,word  = line[0], line[1]\n",
        "      tag=tags[k]\n",
        "      k=k+1\n",
        "      viterbiOutput.write(f\"{idx}\\t{word}\\t{tag}\\n\")\n",
        "    else:\n",
        "      viterbiOutput.write(f\"{line[0]}\\n\")    \n",
        "f.close()\n",
        "viterbiOutput.close()\n"
      ],
      "metadata": {
        "id": "FbmoF220kE9m"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PYVPLhtlAeNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L26ICtxvAePY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5DgazH5FAeS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment - using log probabilities"
      ],
      "metadata": {
        "id": "tKQcv7V2AxRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "def viterbi_decode(words, vocab, transition_params, emission_params):\n",
        "    tags = []\n",
        "    n = len(words)\n",
        "    pi = {}\n",
        "    bp = {}\n",
        "    for i in range(n):\n",
        "        word = words[i]\n",
        "        if word not in vocab:\n",
        "            word = \"<unk>\"\n",
        "        for tag in tagset:\n",
        "            if i == 0:\n",
        "                #pi[f\"{i},{tag}\"] = np.log(transition_params.get(f\"STARTING,{tag}\", 1e-6)) + np.log(emission_params.get(f\"{tag},{word}\", 1e-6))\n",
        "                pi[f\"{i},{tag}\"] = np.log(transition_params.get(f\"STARTING,{tag}\", 1e-9)) + np.log(emission_params.get(f\"{tag},{word}\", 1e-9))\n",
        "\n",
        "            else:\n",
        "                #max_prob = -1e10\n",
        "                max_prob = -np.inf\n",
        "                max_prev_tag = None\n",
        "                for prev_tag in tagset:\n",
        "                    #prob = pi[f\"{i-1},{prev_tag}\"] + np.log(transition_params.get(f\"{prev_tag},{tag}\", 1e-6)) + np.log(emission_params.get(f\"{tag},{word}\", 1e-6))\n",
        "                    prob = pi[f\"{i-1},{prev_tag}\"] + np.log(transition_params.get(f\"{prev_tag},{tag}\", 1e-9)) + np.log(emission_params.get(f\"{tag},{word}\", 1e-9))\n",
        "                    if prob > max_prob:\n",
        "                        max_prob = prob\n",
        "                        max_prev_tag = prev_tag\n",
        "                pi[f\"{i},{tag}\"] = max_prob\n",
        "                bp[f\"{i},{tag}\"] = max_prev_tag\n",
        "\n",
        "    #max_prob = -1e10\n",
        "    max_prob = -np.inf\n",
        "\n",
        "    max_end_tag = None\n",
        "    for tag in tagset:\n",
        "        #prob = pi[f\"{n-1},{tag}\"] + np.log(transition_params.get(f\"{tag},STOPPING\", 1e-6))\n",
        "        prob = pi[f\"{n-1},{tag}\"] + np.log(transition_params.get(f\"{tag},STOPPING\", 1e-9))\n",
        "\n",
        "        if prob > max_prob:\n",
        "            max_prob = prob\n",
        "            max_end_tag = tag\n",
        "\n",
        "    tags = [max_end_tag]\n",
        "    for i in range(n-1, 0, -1):\n",
        "        tags.insert(0, bp[f\"{i},{tags[0]}\"])\n",
        "    return tags\n",
        "\n",
        "def evaluate_viterbi_decoding(dev_file, vocab, transition_params, emission_params):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    words = []\n",
        "    gold_tags = []\n",
        "    with open(dev_file, 'r') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            line = line.strip().split('\\t')\n",
        "            if len(line) == 3:\n",
        "                word, gold_tag = line[1], line[2]\n",
        "                words.append(word)\n",
        "                gold_tags.append(gold_tag)\n",
        "            #else:\n",
        "            #    words = [word for word in words if word in vocab]\n",
        "    tags = viterbi_decode(words, vocab, transition_params, emission_params)\n",
        "    for j in range(len(words)):\n",
        "        if tags[j] == gold_tags[j]:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "dev_file = 'dev'\n",
        "vocab_file = 'vocab.txt'\n",
        "model_file = 'hmm.json'\n",
        "\n",
        "with open(vocab_file, 'r') as f:\n",
        "    vocab = {line.strip().split('\\t')[0]: int(line.strip().split('\\t')[2]) for line in f}\n",
        "\n",
        "with open(model_file, 'r') as f:\n",
        "    model = json.load(f)\n",
        "    transition_params = model['transition']\n",
        "    emission_params = model['emission']\n",
        "\n",
        "accuracy = evaluate_viterbi_decoding(dev_file, vocab, transition_params, emission_params)\n",
        "print(f\"Accuracy on dev data: {accuracy}\")\n"
      ],
      "metadata": {
        "id": "9SHcbieaAkTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Accuracy on dev data: 0.9461705421650173\n"
      ],
      "metadata": {
        "id": "vlqwt1alAoCN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}